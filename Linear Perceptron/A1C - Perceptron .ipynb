{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jalaj Bansal, Manisha Kataria, Aditya Agarwal\n",
    "### A1 - C\n",
    "### Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset LP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x0       x1      x2      x3       x4  t\n",
      "0   1  3.62160  8.6661 -2.8073 -0.44699  0\n",
      "1   1  4.54590  8.1674 -2.4586 -1.46210  0\n",
      "2   1  3.86600 -2.6383  1.9242  0.10645  0\n",
      "3   1  3.45660  9.5228 -4.0112 -3.59440  0\n",
      "4   1  0.32924 -4.4552  4.5718 -0.98880  0\n"
     ]
    }
   ],
   "source": [
    "# Read data and normalize if required\n",
    "\n",
    "df = pd.read_csv('dataset_LP_1.txt',sep=',',header=None)\n",
    "df.columns = ['x1','x2','x3','x4','t']\n",
    "df.insert(0,'x0',1)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "def normalize(ds):\n",
    "    for itr in range(ds.shape[1]):\n",
    "        if ds.columns[itr] == 't' or ds.columns[itr] == 'x0' : continue\n",
    "        original = ds.iloc[:,itr]\n",
    "        ds.iloc[:,itr] = (original - np.mean(original))/np.std(original)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our perceptron model is w0 + w1x1 + w2x2 + w3x3 + w4x4, if the equation throws activation>0 then t = 1 else t = 0\n",
    "#### 1. We normalise our dataset (OPTIONAL)\n",
    "#### 2. We split the data into 70:30 train:test\n",
    "#### 3. We build our hypthesis by learning the weights (parameters) by applying Stochastic Gradient descent\n",
    "#### 4. Using the parameters we make predictions and report accuracy on test data as % correctly classified / total samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset LP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x0        x1        x2        x3  t\n",
      "0   1 -6.672418 -1.206198 -1.081050  0\n",
      "1   1  1.675598  0.614994 -0.971600  0\n",
      "2   1 -4.039058  0.335102  0.544618  1\n",
      "3   1  0.793526 -0.235277  0.551771  1\n",
      "4   1  3.820273 -0.274691  0.454743  1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset_LP_2.csv',sep=',',header=None)\n",
    "df.columns = ['x1','x2','x3','t']\n",
    "df.insert(0,'x0',1)\n",
    "print(df.head())\n",
    "\n",
    "def normalize(ds):\n",
    "    for itr in range(ds.shape[1]):\n",
    "        if ds.columns[itr] == 't' or ds.columns[itr] == 'x0' : continue\n",
    "        original = ds.iloc[:,itr]\n",
    "        ds.iloc[:,itr] = (original - np.mean(original))/np.std(original)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our perceptron model is w0 + w1x1 + w2x2 + w3x3, if the equation throws activation>0 then t = 1 else t = 0\n",
    "#### 1. We normalise our dataset (OPTIONAL)\n",
    "#### 2. We split the data into 70:30 train:test\n",
    "#### 3. We build our hypthesis by learning the weights (parameters) by applying Stochastic Gradient descent\n",
    "#### 4. Using the parameters we make predictions and report accuracy on test data as % correctly classified / total samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning the weights with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "No of iterations:  55\n",
      "Training error:  0.0\n",
      "Testing accuracy : 100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Misclassified points / Total points\n",
    "def cost_find(w,y,x):\n",
    "    n = len(y)\n",
    "    y = y[:,np.newaxis]\n",
    "    y_pred = x @ w\n",
    "    misclass = 0\n",
    "    for i in range(0,len(y_pred)):\n",
    "        if y_pred[i]>0:\n",
    "            y_pred[i]=1\n",
    "        else:\n",
    "            y_pred[i]=0\n",
    "        if y_pred[i]!=y[i]:\n",
    "            misclass += 1\n",
    "    #print(\"Total Misclassified : \",misclass)\n",
    "    cost = (misclass/n) * 100.0\n",
    "    #print(\"% Accuracy : \", 100-cost)\n",
    "    return cost\n",
    "\n",
    "# Since this is Stochastic, it picks a random point from the training data and checks if it is correctly classified\n",
    "# w(t+1)= w(t) + learning_rate * (expected(t) - predicted(t)) * x(t)\n",
    "def SCostFunction(w, x, y):\n",
    "    n = len(y)\n",
    "    y = y[:,np.newaxis]\n",
    "    # random sampling\n",
    "    i = np.random.choice(np.arange(len(y)))\n",
    "    x_temp = x[i]\n",
    "    x_temp = x_temp.reshape(1,x_temp.shape[0]) # xT\n",
    "    y_pred = x_temp @ w # xTw\n",
    "    if y_pred > 0:\n",
    "        y_pred = 1\n",
    "    else :\n",
    "        y_pred = 0\n",
    "    x_temp = x_temp.reshape(x_temp.shape[1],1)\n",
    "    y1 = y_pred - y[i]\n",
    "    gradient = (x_temp @ y1) # expected t - predicted t * x\n",
    "    gradient = gradient.reshape(gradient.shape[0],1)\n",
    "    return gradient\n",
    "\n",
    "\n",
    "# perform 1000000 iterations where weight is updated on misclassification\n",
    "# stop early if zero misclassified points\n",
    "def Stochastic_GD(w,x,y,epochs,eta,threshold):\n",
    "    n=len(y)\n",
    "    itrs=0\n",
    "    costs = []\n",
    "    while True:\n",
    "        gradient = SCostFunction(w,x,y)\n",
    "        cost = cost_find(w,y,x)\n",
    "        w = w - (eta * gradient) # update weights - for correct class there is no change as gradient is zero\n",
    "        itrs=itrs+1\n",
    "        if itrs%100 == 0:\n",
    "            print(\"No of iterations: \",itrs)\n",
    "            print(\"Training error: \",cost)    \n",
    "        if itrs==epochs  or cost==0.0:\n",
    "            print(\"No of iterations: \",itrs)    \n",
    "            print(\"Training error: \",cost)\n",
    "            return cost,w\n",
    "        costs.append(cost)\n",
    "\n",
    "# helper        \n",
    "def find_weights(x_train,y_train,initial_w,eta,epochs,threshold):\n",
    "    tcost,final_w = Stochastic_GD(initial_w,x_train,y_train,epochs,eta,threshold)\n",
    "    return final_w\n",
    "\n",
    "\n",
    "# 70:30 train:test split\n",
    "def split_data(df):\n",
    "    train,test = np.split(df.sample(frac=1).reset_index(drop=True),[int(.7*len(df))])\n",
    "    return train,test\n",
    "\n",
    "# start perceptron with input parameters\n",
    "def perceptron(dataset,learning_rate,norm,epochs,threshold):\n",
    "    if norm==1:\n",
    "        dataset = normalize(dataset)\n",
    "    train,test = split_data(dataset)\n",
    "    print(train.shape[1])\n",
    "    x_train = np.array(train.iloc[:,0:train.shape[1]-1])\n",
    "    y_train = np.array(train.iloc[:,train.shape[1]-1])\n",
    "    x_test = np.array(test.iloc[:,0:train.shape[1]-1])\n",
    "    y_test = np.array(test.iloc[:,train.shape[1]-1])\n",
    "    # initialize random weights\n",
    "    initial_w = np.random.rand(train.shape[1]-1,1)\n",
    "    # print(initial_w)\n",
    "    final_w = find_weights(x_train,y_train,initial_w,learning_rate,epochs,threshold)\n",
    "    # print(final_w)\n",
    "    test_error = cost_find(final_w,y_test,x_test)\n",
    "    print(\"Testing accuracy :\",100.0-test_error,\"%\")\n",
    "    return final_w,x_test,y_test\n",
    "\n",
    "# get predicted values for inputs\n",
    "def predict(parameters,x):\n",
    "    y_pred = x @ parameters\n",
    "    for i in range(0,len(y_pred)):\n",
    "        if y_pred[i]>0:\n",
    "            y_pred[i]=1\n",
    "        else:\n",
    "            y_pred[i]=0\n",
    "    return y_pred\n",
    "\n",
    "## begin\n",
    "model_parameters,x_test,y_test = perceptron(df,0.1,1,1000,0)\n",
    "\n",
    "#print(predict(model_parameters,x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
